## Прогнозирование риска ДТП 

В данном проекте показаны реализации моделей для прогнозирования риска аварий. Проект выполнен в рамках Kaggle соревнования "Predicting Road Accident Risk": https://www.kaggle.com/competitions/playground-series-s5e10/overview .

В соревновании Kaggle "Predicting Road Accident Risk" моя лучшая реализация (ансамбль XGBoost и CatBoost) входит в **топ 23%** решений на публичном наборе данных и в **топ 30%** на приватном. 

Основным **подходом** было выбрано последовательное создание baseline-моделей (XGBoost, CatBoost, нейросеть), затем построение ансамблей на их основе.

**Метрика качества**: RMSE.

## Структура репозитория

- `3.xgboost_baseline.ipynb` - базовая модель XGBoost.
- `4.catboost_baseline.ipynb` - базовая модель CatBoost.
- `5.nn_baseline.ipynb` - базовая нейросеть на PyTorch.
- `1.ensemble_catboost_xgboost.ipynb` - ансамбль CatBoost + XGBoost (простое усреднение).
- `2.ensemble_nn_xgboost.ipynb` - ансамбль XGBoost + NN (взвешенное усреднение).

*Примечание: модели в репозитории расположены в возрастающем порядке (от лучшей метрики к худшей)*

## Данные и признаки
- **Размер `train`**: 517754 строк, 14 столбцов.
- **Целевая переменная**: `accident_risk` (тип float, диапазон [0, 1]).
- **Признаки**:
  - **Категориальные**: `road_type`, `lighting`, `weather`, `road_signs_present`, `public_road`, `time_of_day`, `holiday`, `school_season`.
  - **Числовые**: `num_lanes`, `curvature`, `speed_limit`, `num_reported_accidents`.
- **Качество данных**:
  - Пропуски отсутствуют.
  - Столбец `id` используется только как идентификатор и исключается из признаков.

### Предобработка

* **Для XGBoost**:
  - Все категориальные признаки кодируются через `LabelEncoder`.
  - Масштабирование признаков не используется.
* **Для CatBoost**:
  - Категориальные признаки передаются как строки.
  - Список `cat_features` задаётся по именам колонок.
* **Для нейросети (PyTorch)**:
  - Категориальные признаки также кодируются `LabelEncoder`.
  - Только числовые столбцы нормализуются через `StandardScaler`.
  - Масштабирование обучается на train и применяется к val/test.

Валидация во всех ноутбуках построена через разбиение `train_test_split(test_size=0.2, random_state=42)`.

## Baseline-модели

### XGBoost baseline (`3.xgboost_baseline.ipynb`)

- **Модель**: `xgboost.XGBRegressor`.

- **Ключевые гиперпараметры** :
  - `n_estimators ≈ 1000–1200`- количество деревьев (итераций бустинга) в ансамбле, больше деревьев повышает выразительность модели, но увеличивает время обучения и риск переобучения.
  - `max_depth = 8` - максимальная глубина деревьев, контролирует сложность модели и склонность к переобучению.
  - `learning_rate = 0.05` -  шаг градиентного бустинга, низкое значение делает обучение более стабильным, но требует больше деревьев.
  - `subsample = 0.9` - доля строк, случайно выбираемых для обучения каждого дерева, помогает бороться с переобучением.
  - `colsample_bytree = 0.9` - доля признаков, выбираемых для каждого дерева, снижает коррелированность деревьев.
  - `reg_alpha = 0.01` - L1-регуляризация весов модели, помогает предотвратить переобучение и сделать её более простой, побуждая использовать меньше признаков.
  - `reg_lambda = 0.1` - L2-регуляризация, сглаживает модель и уменьшает переобучение.
  - `gamma = 0.01` - минимальное уменьшение функции потерь для разбиения узла, повышает «консервативность» дерева (т.е. меньше разбиений).
  - `tree_method = 'hist'` - быстрый и масштабируемый алгоритм построения деревьев на основе гистограмм.
  - `early_stopping_rounds = 50` - количество итераций без улучшения на валидации, после которых обучение останавливается (защита от переобучения).

- **Результаты на валидации XGBoost**: RMSE = 0.05618
Эта модель даёт лучший результат среди одиночных моделей.

### CatBoost baseline (`4.catboost_baseline.ipynb`)

- **Модель**: `catboost.CatBoostRegressor`.

- **Ключевые гиперпараметры**:
  - `iterations ≈ 2000` - количество деревьев (итераций бустинга) в ансамбле, больше деревьев повышает выразительность модели, но увеличивает время обучения и риск переобучения.
  - `depth ≈ 7` - максимальная глубина деревьев, контролирует сложность модели и склонность к переобучению.
  - `learning_rate ≈ 0.1` - шаг бустинга, более низкое значение делает обучение стабильнее, но требует больше итераций.
  - `l2_leaf_reg ≈ 3`- L2-регуляризация, сглаживает модель и уменьшает переобучение.
  - `subsample ≈ 0.9`- - доля строк, случайно выбираемых для обучения каждого дерева, помогает бороться с переобучением. 
  - `early_stopping_rounds` - количество итераций без улучшения на валидации, после которых обучение останавливается (защита от переобучения).
  
- **Результаты на валидации CatBoost**: RMSE = 0.05624


### NN baseline (PyTorch) (`5.nn_baseline.ipynb`)

- **Модель**: полносвязная нейросеть, архитектура:
  - Последовательность блоков `Linear → BatchNorm1d → ReLU → Dropout` с постепенным сужением размерности (512 → 256 → 128 → 64 → 32 → 16 → 1).
  - На выходе `Sigmoid`, так как целевая переменная вероятность.

- **Обучение**:
  - Функция потерь: `MSELoss`.
  - Оптимизатор: Adam.
  - Early stopping по лучшему `val_loss`.
  - Батчи через `DataLoader` с размером 512.

- **Результаты на валидации nn**: RMSE = 0.05661

**Вывод по baseline**: XGBoost — наиболее сильная одиночная модель в этом наборе экспериментов, CatBoost немного уступает, нейросеть показывает схожий, но самый слабый результат.

## Ансамбли

### CatBoost + XGBoost, простое усреднение (`1.ensemble_catboost_xgboost.ipynb`)

**Идея**: объединить модели двух бустингов, усреднив их предсказания.

**Схема ансамбля**:
  - Обучение CatBoost и XGBoost на одинаковом разбиении данных.
  - Предсказания на валидации и тесте для каждой модели.
  - Финальное предсказание: `0.5 * CatBoost + 0.5 * XGBoost`.

**Результаты на валидации**:

```
              RMSE 
CatBoost    0.05624
XGBoost     0.05618
Ensemble    0.05617
```
Ансамбль даёт небольшое, но стабильное улучшение RMSE по сравнению с каждой из одиночных моделей.

### XGBoost + NN, взвешенное усреднение (`2.ensemble_nn_xgboost.ipynb`)

**Идея**: добавить в ансамбль модель другого типа (нейросеть), чтобы захватить иной тип зависимостей.

**Шаги**:
  - Обучение XGBoost на закодированных признаках `X_encoded`.
  - Обучение нейросети на нормализованной версии этих же признаков.
  - Получение предсказаний двух моделей.
  - Перебор нескольких пар весов `(w_xgb, w_nn)` (от равных (0.5, 0.5) до смещения в сторону XGBoost (0.7, 0.3) и NN (0.4, 0.6)).

**Лучший результат** получен при весах `XGB = 0.70`, `NN = 0.30`:

```
            RMSE
XGBoost    0.05618
NN         0.05661
Ensemble   0.05620
```
Ансамбль с нейросетью не улучшает результат относительно XGBoost. Это показывает, что добавление сложной NN-модели не всегда даёт выигрыш, если бустинг уже хорошо описывает структуру данных.

## Итоговые метрики 

```
                                 RMSE 
XGBoost baseline                0.05618
CatBoost baseline               0.05624
NN baseline                     0.05661
Ensemble CatBoost+XGBoost       0.05617
Ensemble XGBoost+NN             0.05620
```
На основе сравнения моделей можно сделать вывод, что лучшей одиночной моделью является XGBoost, CatBoost и нейросеть немного ему уступают, а наилучший результат достигается с помощью простого ансамбля CatBoost+XGBoost, который даёт небольшое улучшение RMSE.
